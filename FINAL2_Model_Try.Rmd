---
title: "FINAL_Model_Try"
output: html_document
date: "2024-12-11"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# 1. load packages



```{r}
library(dplyr)
library(corrplot) # Calculate the correlation matrix
library(VIM) # Nearest Neighbor Interpolation of Missing Values
library(ggplot2)
library(cluster)
library(nnet) # multiclassification logistic regression
library(caret)
library(randomForest)
library(glmnet)
#library(BART)
#library(bartMachine)

library(dbarts)
```


# 2. load data


```{r}
### READ ME!!!
# THIS data was generated by FINAL_Data_Clean.RMD
###


data <- read.csv("merged_data_all/merged_data_all.csv")
```

# 3. Data cleaning

## 3.1. Split by NBA 3-point line time

```{r}
# Ensure that date columns are converted to date format
data <- data %>%
  mutate(injury_date = as.Date(injury_date, format = "%Y-%m-%d"))

# Split data
data_before <- data %>% filter(injury_date < as.Date("1979-10-12"))
data0 <- data %>% filter(injury_date >= as.Date("1979-10-12"))
```

## 3.2. Split data

### 3.2.1. Remove rows with too many NAs

```{r}
# Count the number of NAs in each row
na_count <- rowSums(is.na(data0))

# Set the threshold, e.g. number of NA's greater than x
threshold <- 5

# Filter rows with too much NA (keep rows with little NA)
filtered_data <- data0[na_count <= threshold, ]
```

### 3.2.1. Assign ‘other injury’ types

```{r}
# Conditional assignment: 
# replace Col2 with the value of Col1 when Col1 has a value, 
# and keep the original value of Col2 when Col1 is NA.
data0$Injury_Method <- ifelse(is.na(data0$Other_Injury), data0$Injury_Method, data0$Other_Injury)
data0$Injury_Location <- ifelse(is.na(data0$Other_Injury), data0$Injury_Location, data0$Other_Injury)
```

### 3.2.2. Disaggregate data on mode of injury and location of injury

```{r}
# Filtering rows with values
data_method <- data0 %>%
  filter(!is.na(Injury_Method)) %>% 
  select(-Injury_Location, -Other_Injury)

data_location <- data0 %>%
  filter(!is.na(Injury_Location)) %>% 
  select(-Injury_Method, -Other_Injury)
```


### 3.2.4. Drop useless columns 

```{r}
method_season_data <- data_method %>%
  select(-birth_date,-injury_date, 
         -Player_Name_Cleaned, -age_at_injury, 
         -height_cm, -weight_kg, 
         -Team, -position, -Injury_Method, -career_before, -career_after, -career_all,
         -Notes, -Season,
         -injury_year, -injury_month, -injury_day, -birth_year, -birth_month, -birth_day,
         -year_start, -year_end,
         )

location_season_data <- data_location %>%
  select(-birth_date,-injury_date, 
         -Player_Name_Cleaned, -age_at_injury, 
         -height_cm, -weight_kg, 
         -Team, -position, -Injury_Location, -career_before, -career_after, -career_all,
         -Notes, -Season,
         -injury_year, -injury_month, -injury_day, -birth_year, -birth_month, -birth_day,
         -year_start, -year_end
         )
```

### 3.2.5. Deletion of excessive NA data

```{r}
# Count the number of NA values per line
na_count <- rowSums(is.na(method_season_data))

# Setting thresholds (rows with NA values over 20 need to be deleted)
rows_to_remove <- which(na_count > 20)

# Remove rows with too many NA values
cleaned_method_season <- method_season_data[-rows_to_remove, ]

# Delete the corresponding line from data_method
cleaned_data_method <- data_method[-rows_to_remove, ]
```

```{r}
# repeat for location
na_count <- rowSums(is.na(location_season_data))

# 
rows_to_remove <- which(na_count > 20)

# 
cleaned_location_season <- location_season_data[-rows_to_remove, ]

# 
cleaned_data_location <- data_location[-rows_to_remove, ]
```


### 3.2.6. complement missing value value

```{r}
# nearest neighbor complement
cleaned_method_season <- kNN(cleaned_method_season, k = 5)
```


```{r}
# 
cleaned_location_season <- kNN(cleaned_location_season, k = 5)
```

```{r}
# Delete all columns ending with _imp
cleaned_method_season <- cleaned_method_season[, !grepl("_imp$", colnames(cleaned_method_season))]
```


```{r}
# 
cleaned_location_season <- cleaned_location_season[, !grepl("_imp$", colnames(cleaned_location_season))]
```


### 3.2.8. Merg3 cleaned data

```{r}
cleaned_method_others <- cleaned_data_method %>% 
  select(birth_date, injury_date, Player_Name_Cleaned, 
         age_at_injury, height_cm, weight_kg, 
         Team, position, Injury_Method, career_before, career_after
         )


cleaned_location_others <- cleaned_data_location %>% 
  select(birth_date, injury_date, Player_Name_Cleaned, 
         age_at_injury, height_cm, weight_kg, 
         Team, position, Injury_Location, career_before, career_after
         )
```

```{r}
cleaned_all_method <- cbind(cleaned_method_others, cleaned_method_season)
cleaned_all_location <- cbind(cleaned_location_others, cleaned_location_season)
```


```{r}
anyNA(cleaned_all_method)
anyNA(cleaned_all_location)
```


# 4. Correlation analysis

## 4.1. Correlation matrix
```{r}
cor_matrix <- cor(cleaned_method_season, use = "pairwise.complete.obs")
corrplot(cor_matrix, 
         method = "circle", 
         type = "lower", 
         tl.col = "black",   # label color
         tl.srt = 45,        # Label Rotation Angle
         tl.cex = 0.3       # Label text size (adjusted to 0.3x size)
)
```

## 4.2. Highly correlated variables have a significant impact on

```{r}
threshold <- 0.8  # correlation threshold

# Convert correlation matrix to long format (remove diagonal)
high_corr <- which(abs(cor_matrix) > threshold & row(cor_matrix) != col(cor_matrix), arr.ind = TRUE)

# Create a data frame to store the results
high_corr_df <- data.frame(
  Var1 = rownames(cor_matrix)[high_corr[, 1]],  # first variable
  Var2 = colnames(cor_matrix)[high_corr[, 2]],  # second variable
  Correlation = cor_matrix[high_corr]          # correlation value
)

print(high_corr_df)
```

# #FOLLOWING ONLY FOR LOCATION#

# 5. Non-numeric vs. numeric

```{r}
numeric_location <- cleaned_all_location %>% 
  select(-birth_date, -injury_date, 
         -Player_Name_Cleaned, -Injury_Location,
         -Team, -position, - career_after)
```

```{r}
numeric_method <- cleaned_all_method %>% 
  select(-birth_date, -injury_date, 
         -Player_Name_Cleaned, -Injury_Method,
         -Team, -position, - career_after)
```


```{r}
location_category_var <- cleaned_all_location %>% 
  select(Injury_Location, Team, position)
```

# 6. Kmeans


## 6.1. Standardize data
```{r}
# Standardized data (mean subtracted by column, divided by standard deviation)
location_scaled <- scale(location_numeric)
```


## 6.2. Determine the optimal K value

### 6.2.1. WCSS Elbow Rule


```{r}
# Use the elbow rule to plot WCSS (Total Cluster Sum of Squares) against the value of K
wcss <- numeric()
for (k in 1:10) {  # Test 1 to 10 clusters
  kmeans_result <- kmeans(location_scaled, centers = k, nstart = 25)
  wcss[k] <- kmeans_result$tot.withinss
}

plot(1:10, wcss, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters K",
     ylab = "Total Within-Cluster Sum of Squares (WCSS)")
```


### 6.2.2. Average profile factor（Silhouette Method）

```{r}
sil_width <- numeric()
for (k in 2:10) {
  kmeans_result <- kmeans(location_scaled, centers = k, nstart = 25)
  sil <- silhouette(kmeans_result$cluster, dist(location_scaled))
  sil_width[k] <- mean(sil[, 3])
}

# Plot the change in average profile width with K
plot(2:10, sil_width[-1], type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters K",
     ylab = "Average Silhouette Width")
```



## 6.3. K-Means 

```{r}

k <- 4  # Number of clusters selected
kmeans_result <- kmeans(location_scaled, centers = k, nstart = 25)

# View clustering results
#print(kmeans_result$centers)   
#print(kmeans_result$cluster)   
```


# 7. PCA and Visualization

## 7.1. natural clustering

```{r}

#  (PCA)
pca_result <- prcomp(location_scaled, scale. = TRUE)

# Extracting the scores of the first two principal components
pc_scores <- as.data.frame(pca_result$x[, 1:2])  # 提取 PC1 和 PC2
colnames(pc_scores) <- c("PC1", "PC2")

# 3. Add clustering results to PCA data frame
pc_scores$Cluster <- as.factor(kmeans_result$cluster)  # 聚类结果（需要先运行 K-Means）

# 4. Plotting Scatter Plots
ggplot(pc_scores, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point(size = 1, alpha = 0.6) +
  labs(title = "K-Means Clustering with PCA",
       x = "Principal Component 1 (PC1)",
       y = "Principal Component 2 (PC2)") +
  theme_minimal()
```

## 7.2. By Location Injury, method and postion

```{r}
#Location and method, no obvious difference

location_plot <- cbind(pc_scores, location_category_var)

ggplot(location_plot, aes(x = PC1, y = PC2, color = position)) +
  geom_point(size = 1, alpha = 0.4) + # Adjusting transparency alpha
  labs(title = "K-Means Clustering with PCA",
       x = "Principal Component 1 (PC1)",
       y = "Principal Component 2 (PC2)") +
  theme_minimal()


```

# 8. multi-logistic regression 

## 8.1. for Injury_Location

### 8.1.1. Merge Injury_Location category

```{r}
# counts per category
injury_counts <- as.data.frame(table(location_plot$Injury_Location))

# Sort by number in descending order
injury_counts <- injury_counts[order(injury_counts$Freq, decreasing = TRUE), ]

# Rename column names to more intuitive names
colnames(injury_counts) <- c("Injury_Location", "Count")

print(injury_counts)
```


```{r}
Injury_Location_df <- location_category_var %>% 
  select(Injury_Location) %>% 
  mutate(Injury_Location = case_when(
    Injury_Location %in% c("Arm Issues", 
                           "Wrist Issues", 
                           "Shoulder Issues",
                           "Elbow Issues",
                           "Hand Issues"
                           ) ~ "Other Upper Limb Issues",
    Injury_Location %in% c("Tendon Issues",
                           "Hip Issues"
                           ) ~ "Other Lower Limb Issues",
    Injury_Location %in% c("Neck Issues", 
                           "Other",
                           "Head Issues"
                           ) ~ "Other Issues",
    TRUE ~ Injury_Location  # 其他类别保持不变
  ))
  
```

### 8.1.2. lcation data for l.R.


```{r}
Location_data_lr <- cbind(location_numeric, Injury_Location_df)
# Ensure that the target variable for the categorization task is the factor type
Location_data_lr$Injury_Location <- as.factor(Location_data_lr$Injury_Location)
```

### 8.1.3. Training set and test set

```{r}
# Randomly generated indexes for the training set
train_indices <- sample(1:nrow(Location_data_lr), size = 0.7 * nrow(Location_data_lr))

# Divide the data set
Location_lr_train <- Location_data_lr[train_indices, ]  
Location_lr_test <- Location_data_lr[-train_indices, ]  


cat("Size of Training data:", nrow(Location_lr_train), "\n")
cat("Size of Test data:", nrow(Location_lr_test), "\n")

```


### 8.1.4. Fit model

```{r}
# Fit model
multi_logit_model <- multinom(Injury_Location ~ ., data = Location_lr_train)

#summary(multi_logit_model)
```
### 8.1.5. test model

```{r}
# Prediction using test sets
predictions <- predict(multi_logit_model, newdata = Location_lr_test)

```

```{r}
# Generate Confusion Matrix
confusion_matrix <- table(Location_lr_test$Injury_Location, predictions)

# Calculate overall accuracy
accuracy <- sum(diag(confusion_matrix)) / sum(confusion_matrix)
cat("Overall Accuracy:", accuracy, "\n")
```



# 9. random forest

```{r}
# Fitting a Random Forest Model
rf_model <- randomForest(Injury_Location ~ ., data = Location_lr_train, ntree = 100, mtry = 2, importance = TRUE)

# predictions
predictions <- predict(rf_model, newdata = Location_lr_test)

# Evaluation model
accuracy <- mean(predictions == Location_lr_test$Injury_Location)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

# Reduce font size to 50%
varImpPlot(rf_model, main = "Variable Importance", cex = 0.5)
```



10. BART(Not be used )
10.1. Prepare data
10.1.2. PC data
```{r}
# 2. PCA
pca_result <- prcomp(location_scaled, scale. = TRUE)

# Extracting the scores of the first two principal components
pc_scores_BART <- as.data.frame(pca_result$x[, 1:20])  #extract the first 20 PCs
colnames(pc_scores_BART) <- c("PC1", "PC2", "PC3", "PC4", "PC5",
                         "PC6", "PC7", "PC8", "PC9", "PC10",
                         "PC11", "PC12", "PC13", "PC14", "PC15",
                         "PC16","PC17", "PC18", "PC19", "PC20")

pc_BART <- cbind(Injury_Location_df, pc_scores_BART)
```

10.1.3. 分数据

```{r}
# # Randomly generated indexes for the training set
# train_indices <- sample(1:nrow(pc_BART), size = 0.7 * nrow(pc_BART))
# 
# Location_BART_train <- pc_BART[train_indices, ]  
# Location_BART_test <- pc_BART[-train_indices, ]  
# 
# cat("Training set size:", nrow(Location_BART_train), "\n")
# cat("Testing set size:", nrow(Location_BART_test), "\n")
# 
# ```
# 
# 
# ```{r}
# # Use of the One-vs-Rest technique (splitting into multiple dichotomous tasks for multiple classifications)
# 
# # Iterate over all categories
# unique_classes <- unique(Location_BART_train$Injury_Location)
# 
# # Initialize the probability prediction matrix
# prob_matrix <- data.frame(matrix(0, nrow = nrow(Location_BART_test), ncol = length(unique_classes)))
# colnames(prob_matrix) <- unique_classes
# 
# # Constructing a One-vs-Rest model for each category
# for (class in unique_classes) {
#   # Create dichotomous target variables: current category vs other categories
#   y_binary <- ifelse(Location_BART_train$Injury_Location == class, 1, 0)
#   
#   # Training the BART model
#   bart_model <- bartMachine(
#     X = Location_BART_train %>% select(-Injury_Location),
#     y = as.factor(y_binary),
#     num_trees = 20,
#     num_burn_in = 1000,
#     num_iterations_after_burn_in = 2000
#   )
#   
#   # Predict the probability that the test set belongs to the current category
#   prob_matrix[, class] <- predict(bart_model, Location_BART_test %>% select(-Injury_Location), type = "prob")
# }
# 
# # The category corresponding to the maximum value of the probability is taken as the final prediction
# final_predictions <- colnames(prob_matrix)[apply(prob_matrix, 1, which.max)]
```



# 10. Conversion to binary data

## 10.1. location


```{r}
# Get all unique injury types
unique_locations <- unique(Injury_Location_df$Injury_Location)

# Initialize the new data frame
Binary_Injury_Location <- Injury_Location_df

# Create a binary column for each injury type
for (location in unique_locations) {
  Binary_Injury_Location[[paste0("is_", gsub(" ", "_", location))]] <- 
    ifelse(Injury_Location_df$Injury_Location == location, 1, 0)
}
```

## 10.2. method

```{r}
# Get all unique method types
unique_methods <- unique(cleaned_all_method$Injury_Method)

# Initialize the new data frame
Binary_Injury_Method <- data.frame(Injury_Method = cleaned_all_method$Injury_Method)

# Create a binary column for each injury type
for (method in unique_methods) {
  Binary_Injury_Method[[paste0("is_", gsub(" ", "_", method))]] <- 
    ifelse(cleaned_all_method$Injury_Method == method, 1, 0)
}
```


## 10.3. location_postion

```{r}
unique_location_postion <- unique(cleaned_all_location$position)

# Initialize the new data frame
Binary_location_postion <- data.frame(position = cleaned_all_location$position)

# Create a binary column for each 
for (postion in unique_location_postion) {
  Binary_location_postion[[paste0("is_", gsub(" ", "_", postion))]] <- 
    ifelse(cleaned_all_location$position == postion, 1, 0)
}
```

## 11.4. method_postion

```{r}
unique_method_postion <- unique(cleaned_all_method$position)

# Initialize the new data frame
Binary_method_postion <- data.frame(position = cleaned_all_method$position)

# Create a binary column for each 
for (postion in unique_method_postion) {
  Binary_method_postion[[paste0("is_", gsub(" ", "_", postion))]] <- 
    ifelse(cleaned_all_method$position == postion, 1, 0)
}
```


## 11.5. Rename columns

```{r}
# Add the suffix “_method” to all column names.
colnames(Binary_location_postion) <- paste0(colnames(Binary_location_postion), "_location")
colnames(Binary_method_postion) <- paste0(colnames(Binary_method_postion), "_method")

```

## 11.6. Delete original column

```{r}
Binary_Injury_Location <- Binary_Injury_Location %>% 
  select(-Injury_Location)
Binary_Injury_Method <- Binary_Injury_Method %>% 
  select(-Injury_Method)
Binary_location_postion <- Binary_location_postion %>% 
  select(-position_location)
Binary_method_postion <- Binary_method_postion %>% 
  select(-position_method)

```


```{r}

# write.csv(Binary_Injury_Location, "binary_injury_location.csv", row.names = FALSE)
# write.csv(Binary_Injury_Method, "binary_injury_method.csv", row.names = FALSE)
# write.csv(Binary_location_postion, "binary_location_position.csv", row.names = FALSE)
# write.csv(Binary_method_postion, "binary_method_position.csv", row.names = FALSE)
# write.csv(numeric_location, "numeric_location.csv", row.names = FALSE)
# write.csv(numeric_method, "numeric_method.csv", row.names = FALSE)

```





# #TAKE KNEE INJURY AS EXAMPLE#
# 11. BART

## 11.1. Merge data Split data

```{r}
BART_data_og <- cbind(Injury_Location_binary, location_numeric)
```


```{r}
train_indices <- sample(1:nrow(BART_data_og), size = 0.7 * nrow(BART_data_og))


Location_BART_train_og <- BART_data_og[train_indices, ]  
Location_BART_test_og <- BART_data_og[-train_indices, ]  

cat("Train size:", nrow(Location_BART_train_og), "\n")
cat("Test size:", nrow(Location_BART_test_og), "\n")

```

## 11.2. BART

```{r}
# # Building the BART Model
# bart_model <- bartMachine(
#   X = Location_BART_train_og %>% select(colnames(location_numeric)),  # Characterization variables (removing the target variable)
#   y = as.factor(Location_BART_train_og$is_Knee_Issues),   # binary target variable
#   num_trees = 50,               
#   num_burn_in = 1000,           
#   num_iterations_after_burn_in = 2000  
# )
# 
# # View model results
# summary(bart_model)
# 

# Use dbarts Building the BART Model
bart_model <- dbarts::bart(
  x.train = Location_BART_train_og %>% select(colnames(location_numeric)),
  y.train = Location_BART_train_og$is_Knee_Issues,
  x.test = Location_BART_test_og %>% select(colnames(location_numeric)),
  keeptrees = TRUE  # Must be set to TRUE to make predictions
)


```

## 11.3. predictions
```{r}
# predictions results
predictions <- predict(bart_model, newdata = Location_BART_test_og %>% select(colnames(location_numeric)))
```

```{r}
# Transpose the matrix so that each column corresponds to a test sample
predictions_transposed <- t(predictions)

# Take the mean (the predicted value for each test sample) by row for the transposed matrix
predictions_mean <- rowMeans(predictions_transposed)
```

```{r}
threshold <- 0.5
# Conversion to binary classification results based on probability values
predicted_classes <- ifelse(predictions_mean > threshold, 1, 0)

```

```{r}
# Convert to factor type
predicted_classes <- as.factor(predicted_classes)
actual_classes <- as.factor(Location_BART_test_og$is_Knee_Issues)
```


```{r}
# Generate Confusion Matrix
confusion_matrix <- table(Predicted = predicted_classes, Actual = actual_classes)

# Calculation accuracy
accuracy <- mean(predicted_classes == actual_classes)
print(paste("Accuracy:", round(accuracy * 100, 2), "%"))

```


# 12. Logistic Regression


```{r}
# Extraction of features and target variables
X_train <- Location_BART_train_og %>% select(colnames(location_numeric))
y_train <- as.factor(Location_BART_train_og$is_Knee_Issues)

X_test <- Location_BART_test_og %>% select(colnames(location_numeric))
y_test <- as.factor(Location_BART_test_og$is_Knee_Issues)

```


```{r}
# Fitting logistic regression models
logistic_model <- glm(y_train ~ ., data = X_train, family = binomial)

# Test
logistic_probs <- predict(logistic_model, newdata = X_test, type = "response")
logistic_preds <- ifelse(logistic_probs > 0.5, 1, 0)

# Calculation accuracy
logistic_accuracy <- mean(logistic_preds == y_test)
print(paste("Logistic Regression Accuracy:", round(logistic_accuracy * 100, 2), "%"))

```






